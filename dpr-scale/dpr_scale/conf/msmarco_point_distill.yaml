defaults:
  - config
  - override task: dpr_pair_distill
  - override trainer: gpu_1_host
  - override task/optim: adamw

task:
  shared_model: false
  in_batch_eval: true
  use_pair_scores: false
  pair_loss_alpha: 0.0
  softmax_temperature: 10.0
  optim:
    lr: 3e-5
  warmup_steps: 10000

datamodule:
  _target_: dpr_scale.datamodule.dpr.DenseRetrieverMultiJsonlPairDistillDataModule
  corpus_path: /work/cwhuang0921/fine-grained-distillation/data/msmarco/collection_dpr.tsv
  queries_path: /work/cwhuang0921/fine-grained-distillation/data/msmarco/queries.train.tsv
  train_path: [/work/cwhuang0921/fine-grained-distillation/data/msmarco/official_neg63_point_mini_pair_r2-4_train_800k.jsonl]
  val_path: /work/cwhuang0921/fine-grained-distillation/data/msmarco/official_neg63_point_mini_pair_r2-4_dev.jsonl
  test_path: /work/cwhuang0921/fine-grained-distillation/data/msmarco/official_neg63_point_mini_pair_r2-4_dev.jsonl
  batch_size: 4
  n_passages: 64
  drop_last: false
  use_title: false

trainer:
  gpus: 4
  num_nodes: 1
  max_epochs: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 10
  val_check_interval: 0.25
  gradient_clip_val: 2.0
  precision: 16
  strategy: ddp

checkpoint_callback:
  monitor: valid_loss
  mode: min
  every_n_train_steps: 10000